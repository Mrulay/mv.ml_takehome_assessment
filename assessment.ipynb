{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('labels.csv')\n",
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20795</th>\n",
       "      <td>20795</td>\n",
       "      <td>Rapper T.I.: Trump a ’Poster Child For White S...</td>\n",
       "      <td>Jerome Hudson</td>\n",
       "      <td>Rapper T. I. unloaded on black celebrities who...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20796</th>\n",
       "      <td>20796</td>\n",
       "      <td>N.F.L. Playoffs: Schedule, Matchups and Odds -...</td>\n",
       "      <td>Benjamin Hoffman</td>\n",
       "      <td>When the Green Bay Packers lost to the Washing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20797</th>\n",
       "      <td>20797</td>\n",
       "      <td>Macy’s Is Said to Receive Takeover Approach by...</td>\n",
       "      <td>Michael J. de la Merced and Rachel Abrams</td>\n",
       "      <td>The Macy’s of today grew from the union of sev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20798</th>\n",
       "      <td>20798</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>Alex Ansary</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20799</th>\n",
       "      <td>20799</td>\n",
       "      <td>What Keeps the F-35 Alive</td>\n",
       "      <td>David Swanson</td>\n",
       "      <td>David Swanson is an author, activist, journa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20800 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1          1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2          2                  Why the Truth Might Get You Fired   \n",
       "3          3  15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4          4  Iranian woman jailed for fictional unpublished...   \n",
       "...      ...                                                ...   \n",
       "20795  20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
       "20796  20796  N.F.L. Playoffs: Schedule, Matchups and Odds -...   \n",
       "20797  20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
       "20798  20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
       "20799  20799                          What Keeps the F-35 Alive   \n",
       "\n",
       "                                          author  \\\n",
       "0                                  Darrell Lucus   \n",
       "1                                Daniel J. Flynn   \n",
       "2                             Consortiumnews.com   \n",
       "3                                Jessica Purkiss   \n",
       "4                                 Howard Portnoy   \n",
       "...                                          ...   \n",
       "20795                              Jerome Hudson   \n",
       "20796                           Benjamin Hoffman   \n",
       "20797  Michael J. de la Merced and Rachel Abrams   \n",
       "20798                                Alex Ansary   \n",
       "20799                              David Swanson   \n",
       "\n",
       "                                                    text  label  \n",
       "0      House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1      Ever get the feeling your life circles the rou...      0  \n",
       "2      Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3      Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4      Print \\nAn Iranian woman has been sentenced to...      1  \n",
       "...                                                  ...    ...  \n",
       "20795  Rapper T. I. unloaded on black celebrities who...      0  \n",
       "20796  When the Green Bay Packers lost to the Washing...      0  \n",
       "20797  The Macy’s of today grew from the union of sev...      0  \n",
       "20798  NATO, Russia To Hold Parallel Exercises In Bal...      1  \n",
       "20799    David Swanson is an author, activist, journa...      1  \n",
       "\n",
       "[20800 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(row): #Utility Function\n",
    "    row = str(row)\n",
    "    row = row.lower()\n",
    "    row = re.sub('[^a-zA-Z]', ' ', row)\n",
    "    row_tokens = word_tokenize(row)\n",
    "    row = (lemmatizer.lemmatize(word) for word in row_tokens if word not in stopwords)\n",
    "    row_processed = ' '.join(row_tokens)\n",
    "    #row_processed = word_tokenize(row_processed)\n",
    "    return row_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['content'] = train.title + ' ' + train.author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.content = train.content.apply(data_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 100\n",
    "w2v = gensim.models.Word2Vec(train.content,\n",
    "                            vector_size=VECTOR_SIZE,\n",
    "                            window=7,\n",
    "                            min_count=1,\n",
    "                            epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vector(row): #Vector util\n",
    "    for i,word in enumerate(row):\n",
    "        if(word in w2v.wv.index_to_key):\n",
    "            row[i] = np.asarray(w2v.wv[word])\n",
    "        else:\n",
    "            row[i] = 0\n",
    "    row = (np.mean(row, axis=0)).tolist()\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mrulay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    }
   ],
   "source": [
    "train.content = train.content.apply(word2vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.content.apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = train.label\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.loc[:, df.columns != 'label'], df.label, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.668894</td>\n",
       "      <td>0.821996</td>\n",
       "      <td>0.321929</td>\n",
       "      <td>-0.958595</td>\n",
       "      <td>-0.065362</td>\n",
       "      <td>0.266006</td>\n",
       "      <td>0.652788</td>\n",
       "      <td>0.271767</td>\n",
       "      <td>-0.350623</td>\n",
       "      <td>1.002040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284387</td>\n",
       "      <td>-0.893949</td>\n",
       "      <td>-0.111707</td>\n",
       "      <td>0.191977</td>\n",
       "      <td>-0.538335</td>\n",
       "      <td>-0.131207</td>\n",
       "      <td>-0.333054</td>\n",
       "      <td>0.761424</td>\n",
       "      <td>-0.496558</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.920960</td>\n",
       "      <td>0.345482</td>\n",
       "      <td>0.231933</td>\n",
       "      <td>-0.253391</td>\n",
       "      <td>0.512607</td>\n",
       "      <td>1.082127</td>\n",
       "      <td>0.264062</td>\n",
       "      <td>0.683654</td>\n",
       "      <td>-1.135681</td>\n",
       "      <td>-0.266714</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.031315</td>\n",
       "      <td>0.023972</td>\n",
       "      <td>0.123042</td>\n",
       "      <td>0.192789</td>\n",
       "      <td>-0.387585</td>\n",
       "      <td>-0.872051</td>\n",
       "      <td>-0.190078</td>\n",
       "      <td>-0.804364</td>\n",
       "      <td>0.790919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.314369</td>\n",
       "      <td>-0.375896</td>\n",
       "      <td>-0.572484</td>\n",
       "      <td>-1.250595</td>\n",
       "      <td>-0.549824</td>\n",
       "      <td>0.169462</td>\n",
       "      <td>-0.712321</td>\n",
       "      <td>0.910779</td>\n",
       "      <td>0.053339</td>\n",
       "      <td>1.258737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535325</td>\n",
       "      <td>-0.833016</td>\n",
       "      <td>-1.276946</td>\n",
       "      <td>-0.610670</td>\n",
       "      <td>-0.160201</td>\n",
       "      <td>-1.404328</td>\n",
       "      <td>0.057129</td>\n",
       "      <td>-0.390538</td>\n",
       "      <td>-1.122155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.911460</td>\n",
       "      <td>-0.481384</td>\n",
       "      <td>1.140141</td>\n",
       "      <td>-0.587918</td>\n",
       "      <td>-0.270819</td>\n",
       "      <td>0.470909</td>\n",
       "      <td>0.091845</td>\n",
       "      <td>1.092736</td>\n",
       "      <td>0.559045</td>\n",
       "      <td>-0.111708</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>0.713855</td>\n",
       "      <td>-0.371541</td>\n",
       "      <td>0.305381</td>\n",
       "      <td>-0.600119</td>\n",
       "      <td>0.382483</td>\n",
       "      <td>0.274246</td>\n",
       "      <td>0.301541</td>\n",
       "      <td>-0.467800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.406740</td>\n",
       "      <td>0.528648</td>\n",
       "      <td>0.077141</td>\n",
       "      <td>0.125268</td>\n",
       "      <td>-0.574987</td>\n",
       "      <td>0.807347</td>\n",
       "      <td>-0.040396</td>\n",
       "      <td>0.195821</td>\n",
       "      <td>-0.580574</td>\n",
       "      <td>0.252547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412093</td>\n",
       "      <td>0.314349</td>\n",
       "      <td>-0.520602</td>\n",
       "      <td>0.275927</td>\n",
       "      <td>0.296997</td>\n",
       "      <td>0.693326</td>\n",
       "      <td>0.456611</td>\n",
       "      <td>0.074491</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20795</th>\n",
       "      <td>0.872482</td>\n",
       "      <td>0.519724</td>\n",
       "      <td>0.607144</td>\n",
       "      <td>-0.356767</td>\n",
       "      <td>-0.758656</td>\n",
       "      <td>1.125608</td>\n",
       "      <td>0.159298</td>\n",
       "      <td>0.135372</td>\n",
       "      <td>-0.565150</td>\n",
       "      <td>1.111445</td>\n",
       "      <td>...</td>\n",
       "      <td>1.837866</td>\n",
       "      <td>-1.225709</td>\n",
       "      <td>-0.543379</td>\n",
       "      <td>0.624787</td>\n",
       "      <td>-0.713669</td>\n",
       "      <td>-0.768185</td>\n",
       "      <td>-1.705534</td>\n",
       "      <td>-0.496877</td>\n",
       "      <td>0.195294</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20796</th>\n",
       "      <td>-0.667301</td>\n",
       "      <td>0.748833</td>\n",
       "      <td>-0.442047</td>\n",
       "      <td>0.308489</td>\n",
       "      <td>0.790697</td>\n",
       "      <td>0.130301</td>\n",
       "      <td>0.011303</td>\n",
       "      <td>0.976521</td>\n",
       "      <td>-0.222467</td>\n",
       "      <td>0.327054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.494846</td>\n",
       "      <td>1.092259</td>\n",
       "      <td>0.097544</td>\n",
       "      <td>0.597184</td>\n",
       "      <td>-0.594477</td>\n",
       "      <td>0.240805</td>\n",
       "      <td>-0.504205</td>\n",
       "      <td>0.579502</td>\n",
       "      <td>0.063569</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20797</th>\n",
       "      <td>-0.384687</td>\n",
       "      <td>0.048353</td>\n",
       "      <td>-0.281788</td>\n",
       "      <td>-0.471755</td>\n",
       "      <td>0.098482</td>\n",
       "      <td>0.745488</td>\n",
       "      <td>-0.108597</td>\n",
       "      <td>1.134383</td>\n",
       "      <td>0.011775</td>\n",
       "      <td>0.032442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093460</td>\n",
       "      <td>0.933567</td>\n",
       "      <td>-0.449934</td>\n",
       "      <td>0.657906</td>\n",
       "      <td>0.298438</td>\n",
       "      <td>-0.161413</td>\n",
       "      <td>-0.360772</td>\n",
       "      <td>-0.708034</td>\n",
       "      <td>0.676193</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20798</th>\n",
       "      <td>0.911517</td>\n",
       "      <td>-0.265299</td>\n",
       "      <td>-0.313204</td>\n",
       "      <td>-0.373709</td>\n",
       "      <td>-0.503450</td>\n",
       "      <td>0.757700</td>\n",
       "      <td>-0.399392</td>\n",
       "      <td>0.723084</td>\n",
       "      <td>0.260886</td>\n",
       "      <td>-0.369040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.385994</td>\n",
       "      <td>0.171494</td>\n",
       "      <td>-0.407586</td>\n",
       "      <td>0.607357</td>\n",
       "      <td>0.155787</td>\n",
       "      <td>-1.198669</td>\n",
       "      <td>0.184077</td>\n",
       "      <td>0.593833</td>\n",
       "      <td>-0.694102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20799</th>\n",
       "      <td>0.234149</td>\n",
       "      <td>0.739633</td>\n",
       "      <td>0.073710</td>\n",
       "      <td>-1.576809</td>\n",
       "      <td>1.261813</td>\n",
       "      <td>-0.812909</td>\n",
       "      <td>-0.708263</td>\n",
       "      <td>0.985855</td>\n",
       "      <td>-0.163026</td>\n",
       "      <td>-0.002489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305614</td>\n",
       "      <td>0.370698</td>\n",
       "      <td>-0.250470</td>\n",
       "      <td>-0.259853</td>\n",
       "      <td>0.011388</td>\n",
       "      <td>-0.617605</td>\n",
       "      <td>0.019914</td>\n",
       "      <td>0.175859</td>\n",
       "      <td>0.506237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20771 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.668894  0.821996  0.321929 -0.958595 -0.065362  0.266006  0.652788   \n",
       "1      1.920960  0.345482  0.231933 -0.253391  0.512607  1.082127  0.264062   \n",
       "2      0.314369 -0.375896 -0.572484 -1.250595 -0.549824  0.169462 -0.712321   \n",
       "3      0.911460 -0.481384  1.140141 -0.587918 -0.270819  0.470909  0.091845   \n",
       "4      0.406740  0.528648  0.077141  0.125268 -0.574987  0.807347 -0.040396   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "20795  0.872482  0.519724  0.607144 -0.356767 -0.758656  1.125608  0.159298   \n",
       "20796 -0.667301  0.748833 -0.442047  0.308489  0.790697  0.130301  0.011303   \n",
       "20797 -0.384687  0.048353 -0.281788 -0.471755  0.098482  0.745488 -0.108597   \n",
       "20798  0.911517 -0.265299 -0.313204 -0.373709 -0.503450  0.757700 -0.399392   \n",
       "20799  0.234149  0.739633  0.073710 -1.576809  1.261813 -0.812909 -0.708263   \n",
       "\n",
       "              7         8         9  ...        91        92        93  \\\n",
       "0      0.271767 -0.350623  1.002040  ...  0.284387 -0.893949 -0.111707   \n",
       "1      0.683654 -1.135681 -0.266714  ... -1.031315  0.023972  0.123042   \n",
       "2      0.910779  0.053339  1.258737  ...  0.535325 -0.833016 -1.276946   \n",
       "3      1.092736  0.559045 -0.111708  ... -0.222222  0.713855 -0.371541   \n",
       "4      0.195821 -0.580574  0.252547  ...  0.412093  0.314349 -0.520602   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "20795  0.135372 -0.565150  1.111445  ...  1.837866 -1.225709 -0.543379   \n",
       "20796  0.976521 -0.222467  0.327054  ...  0.494846  1.092259  0.097544   \n",
       "20797  1.134383  0.011775  0.032442  ...  0.093460  0.933567 -0.449934   \n",
       "20798  0.723084  0.260886 -0.369040  ... -0.385994  0.171494 -0.407586   \n",
       "20799  0.985855 -0.163026 -0.002489  ...  0.305614  0.370698 -0.250470   \n",
       "\n",
       "             94        95        96        97        98        99  label  \n",
       "0      0.191977 -0.538335 -0.131207 -0.333054  0.761424 -0.496558      1  \n",
       "1      0.192789 -0.387585 -0.872051 -0.190078 -0.804364  0.790919      0  \n",
       "2     -0.610670 -0.160201 -1.404328  0.057129 -0.390538 -1.122155      1  \n",
       "3      0.305381 -0.600119  0.382483  0.274246  0.301541 -0.467800      1  \n",
       "4      0.275927  0.296997  0.693326  0.456611  0.074491  0.638095      1  \n",
       "...         ...       ...       ...       ...       ...       ...    ...  \n",
       "20795  0.624787 -0.713669 -0.768185 -1.705534 -0.496877  0.195294      0  \n",
       "20796  0.597184 -0.594477  0.240805 -0.504205  0.579502  0.063569      0  \n",
       "20797  0.657906  0.298438 -0.161413 -0.360772 -0.708034  0.676193      0  \n",
       "20798  0.607357  0.155787 -1.198669  0.184077  0.593833 -0.694102      1  \n",
       "20799 -0.259853  0.011388 -0.617605  0.019914  0.175859  0.506237      1  \n",
       "\n",
       "[20771 rows x 101 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2469,  105],\n",
       "       [ 102, 2517]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "linear_clf = RandomForestClassifier()\n",
    "\n",
    "linear_clf.fit(X_train, y_train)\n",
    "prediction2 = linear_clf.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, prediction2)\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "metrics.confusion_matrix(y_test, prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test['labels'] = labels.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['content'] = test.title + ' ' + test.author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.content = test.content.apply(data_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mrulay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:163: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "c:\\Users\\Mrulay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    }
   ],
   "source": [
    "test.content = test.content.apply(word2vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test.content.apply(pd.Series)\n",
    "df['label'] = labels.label\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1541,  797],\n",
       "       [1060, 1789]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction2 = linear_clf.predict(df.loc[:, df.columns != 'label'])\n",
    "score = metrics.accuracy_score(df.label, prediction2)\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "metrics.confusion_matrix(df.label, prediction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.text = train.text.apply(data_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train['text'], train['label'], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "max_len = 350\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 350)]             0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 350, 50)           1000000   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                29440     \n",
      "                                                                 \n",
      " FC1 (Dense)                 (None, 256)               16640     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " out_layer (Dense)           (None, 1)                 257       \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,046,337\n",
      "Trainable params: 1,046,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='mean_squared_error',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "98/98 [==============================] - 3s 24ms/step - loss: 0.1248 - accuracy: 0.8255 - val_loss: 0.0513 - val_accuracy: 0.9311\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 2s 19ms/step - loss: 0.0456 - accuracy: 0.9408 - val_loss: 0.0552 - val_accuracy: 0.9266\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 2s 19ms/step - loss: 0.0282 - accuracy: 0.9643 - val_loss: 0.0496 - val_accuracy: 0.9388\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 2s 19ms/step - loss: 0.0192 - accuracy: 0.9769 - val_loss: 0.0614 - val_accuracy: 0.9196\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 2s 19ms/step - loss: 0.0128 - accuracy: 0.9845 - val_loss: 0.0562 - val_accuracy: 0.9311\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 2s 17ms/step - loss: 0.0116 - accuracy: 0.9861 - val_loss: 0.0618 - val_accuracy: 0.9256\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 2s 17ms/step - loss: 0.0061 - accuracy: 0.9930 - val_loss: 0.0570 - val_accuracy: 0.9362\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 2s 17ms/step - loss: 0.0060 - accuracy: 0.9925 - val_loss: 0.0559 - val_accuracy: 0.9349\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 2s 17ms/step - loss: 0.0040 - accuracy: 0.9952 - val_loss: 0.0642 - val_accuracy: 0.9263\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 2s 17ms/step - loss: 0.0056 - accuracy: 0.9937 - val_loss: 0.0619 - val_accuracy: 0.9314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x193a374a230>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sequences_matrix,y_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 1s 7ms/step - loss: 0.0602 - accuracy: 0.9325\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(test_sequences_matrix,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test['labels'] = labels.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.text = test.text.apply(data_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tok.texts_to_sequences(test.text)\n",
    "test_sequences_matrix = pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 1s 7ms/step - loss: 0.3731 - accuracy: 0.6185\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(test_sequences_matrix,test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d2485756e6eb87ba585c06644778dbca698c1b162aee96ae5ee3a2ddd11dbf0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
